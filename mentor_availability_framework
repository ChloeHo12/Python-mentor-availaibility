import requests
import pandas as pd
import numpy as np
import core_connection as conn
import datetime

def daily_snapshot() -> pd.DataFrame:

    mentor_schedule = {}
    mentor_json = []

    # Query dataframe from BigQuery
    mentors_only_dataframe = conn.query_bigquery("SELECT \
                                                nylasAccountId as mentor_id,\
                                                name as mentor_name,\
                                                email as mentor_signup_email,\
                                                schedule as mentor_nylas_schedule \
                                                FROM `vts-connect-763bf.dev.techsphere_users_mentors_only` \
                                                "
                                                 )

    # Extract schedule from BigQuery query into a list
    schedule_list = mentors_only_dataframe['mentor_nylas_schedule'].tolist()

    # Extract mentor_general_id from schedule
    for url in schedule_list:
        mentor_general_id = url.split('/')[-1]
        mentor_schedule[url] = mentor_general_id

    # Remove empty keys
    mentor_schedule = {k: v for k, v in mentor_schedule.items() if v}

    # Loop through mentor_schedule, make API calls and aggregate all times into a dataframe
    for schedule,id in mentor_schedule.items():

        url = f'https://api.schedule.nylas.com/schedule/{id}/timeslots?allow_stale=true&locale=en'
        api = requests.get(url)

        if api.status_code == 200 and api.json() != []:
            api_json = api.json()
            for j in api_json:
                j['mentor_nylas_schedule'] = schedule
                j['nylas_status_code'] = 200
            mentor_json.extend(api_json)
        else:
            mentor_json.extend([{'nylas_status_code': api.status_code, 'mentor_nylas_schedule': schedule}])

    api_dataframe = pd.DataFrame(mentor_json)

    # Drop unnecessary columns:
    api_dataframe = api_dataframe.drop(columns=['calendar_id','end', 'host_name'])

    # Convert data type:
    api_dataframe['emails'] = api_dataframe['emails'].str[0]

    # Aggregate data to calculate number of open slots, open slots within 7 days, earliest date and latest slot
    api_dataframe['7_days_from_today'] = pd.to_datetime("today") + pd.Timedelta(7, unit='d')
    api_dataframe['start'] = pd.to_datetime(api_dataframe['start'], unit='s')

    sevenDaysConditions = [
        (api_dataframe['start'] >= pd.to_datetime('today')) & ( api_dataframe['start'] <= api_dataframe['7_days_from_today']),
        (api_dataframe['start'] > api_dataframe['7_days_from_today'])]
    sevenDaysChoices = [1, 0]
    api_dataframe['is_within_7_days'] = np.select(sevenDaysConditions, sevenDaysChoices)

    api_dataframe = (
        api_dataframe.groupby(['account_id', 'emails', 'mentor_nylas_schedule', 'nylas_status_code'], dropna=False)
        .agg(total_open_slots=('start', 'size'), open_slots_next_7_days=('is_within_7_days', 'sum'), earliest_slot=('start', 'min'), latest_slot=('start', 'max'))
        .reset_index())

    # Set open_slots to 0 with fully booked mentors:
    api_dataframe.loc[api_dataframe['account_id'].isnull(), 'total_open_slots'] = 0

    # Convert int columns into date time
    for date in ['earliest_slot', 'latest_slot']:
        api_dataframe[date] = pd.to_datetime(api_dataframe[date],unit='s')

    # Rename columns:
    api_dataframe = api_dataframe.rename(columns = {'account_id': 'nylas_id',
                                                    'emails': 'mentor_booking_email'})

    # Merge above dataframe with original queried dataframe
    api_dataframe = pd.merge(mentors_only_dataframe, api_dataframe
                            ,on = 'mentor_nylas_schedule'
                            ,how = 'outer')

    # Merge columns:
    api_dataframe.nylas_id.fillna(api_dataframe.mentor_id, inplace=True)
    del api_dataframe['mentor_id']

    # Special note for audiences to understand rows:
    conditions = [
      (api_dataframe['nylas_status_code'] == 200.0) & (api_dataframe['mentor_booking_email'].isnull()),
      (api_dataframe['nylas_status_code'].isnull()) & (api_dataframe['total_open_slots'].isnull()),
      (api_dataframe['nylas_status_code'] > 200.0)
      ]
    choices = ['fully booked','incomplete profile','errored nylas schedule link']
    api_dataframe['special_note'] = np.select(conditions, choices, default = '')

    # Fill Nas:
    api_dataframe['total_open_slots'].fillna(0, inplace=True)
    api_dataframe['open_slots_next_7_days'].fillna(0, inplace=True)
    api_dataframe['nylas_status_code'].fillna(404, inplace=True)

    # converting from float to int
    for float_type in ['total_open_slots', 'open_slots_next_7_days','nylas_status_code']:
        api_dataframe[float_type] = api_dataframe[float_type].astype(int)

    #Load dataframe into BigQuery:
    conn.load_everything('prod',api_dataframe,'vts_mentor_availability')

if __name__ == '__main__':
    daily_snapshot()
